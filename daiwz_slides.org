#+INCLUDE: style/config.org

#+NAME: eval_conf
#+BEGIN_SRC emacs-lisp :results none :exports none
(set 'conf-file (concat default-directory "style/config.el"))
(load conf-file)
#+END_SRC

#+TITLE: 一阶逻辑领域知识与机器学习的结合研究
#+AUTHOR: 戴望州
#+DATE: <2019-02-26 一>

# Title slide
#+BEGIN_EXPORT html
<section id="slide-title">
<img src="./figs/dissertation/nju.jpg" style="position:absolute;top:5%;right:5%;width:300px;box-shadow:none" />
<div class="talk-title">
    <p style="font-size:0.8em;color:rgba(0,0,0,0.6)">博士论文答辩汇报</p>
    <h1 class="no-toc-progress">一阶逻辑领域知识<br>与机器学习的结合研究</h1>
</div>
<div class="talk-subtitle">
    <p>Research on Integrating First-Order Logical Domain Knowledge with Machine Learning</p>
</div>
<div class="keyboard-usage">
    <p>(Press <code>?</code> for help, <code>n</code> and <code>p</code> for next and previous slide)</p>
</div>
<div class="talk-author" style="font-size:0.7em">
    <p>&emsp;<b>答辩人</b>： 戴望州<br />
       &emsp;&emsp;<b>专业</b>： 计算机科学与技术<br />
       <b>研究方向</b>： 机器学习<br />
       <b>指导教师</b>： 周志华 教授<br />
    <span class="talk-date">2019-02-26 星期二</span></p>
</div>
</section>
<!--大纲-->
<section id="slide-toc">
<div class="talk-toc">
    <h3 class="no-toc-progress">大纲</h3>
</div>
#+END_EXPORT
- 研究背景
- 第二、三章：一阶逻辑领域知识辅助的机器学习方法
  - 第二章：一种领域知识增广样本的机器学习方法
  - 第三章：一种领域知识辅助约束的机器学习方法
- 第四章：一种机器学习驱动的领域知识精化方法
- 第五章：一种领域知识与机器学习互促结合框架
- 总结
#+REVEAL_HTML: </section>
* 研究背景
** 人工智能
#+BEGIN_EXPORT html
<div class=flexbox-stretch>
    <img class="fragment appear" data-fragment-index=0
src="https://media.boingboing.net/wp-content/uploads/2018/11/hal.gif" height=180 width=auto />
    <img class="fragment appear" data-fragment-index=0 src="https://media.tenor.com/images/14ea2784dbf2458cc2911e5dac4e4024/tenor.gif" height=180 width=auto />
    <img class="fragment appear" data-fragment-index=0 src="https://i.pinimg.com/originals/6c/fc/5c/6cfc5c281108d8ae10f4d6ced8c34dc6.gif" height=180 width=auto />
    <img class="fragment appear" data-fragment-index=2 src="http://5b0988e595225.cdn.sohucs.com/images/20170727/a6fbd258cec24b81aa29da7747a633c9.png" height=180 width=320 />    
    <img src="https://www.citibank.com.sg/citigold/images/insights/infograph-1a.gif" height=180 width=280 />
    <img class="fragment appear" data-fragment-index=3 src="./figs/dissertation/trump.png" height=180 width=380 />
    <img class="fragment appear" data-fragment-index=1 src="https://storage.googleapis.com/deepmind-live-cms/documents/sc2-agent-vis%2520%25281%2529.gif" height=180 width=auto />
    <img class="fragment appear" data-fragment-index=1 src="https://thumbs.gfycat.com/DeliciousWeepyBullfrog-small.gif" height=180 width=340 />
    <img class="fragment appear" data-fragment-index=1 src="https://thumbs.gfycat.com/AmusingConfusedBobwhite-max-1mb.gif" height=180 width=auto />    
</div>
#+END_EXPORT
*** 早期人工智能
{{{begin_div(flexbox)}}}
{{{begin_div(leftcol60)}}}
- 一般问题求解:
  - General Problem Solver, 1957
  - 自动定理证明, 1960s
- 知识工程:
  - 专家系统, 1960s
  - 逻辑程序, 1970s
  - "五代机", 1980s
{{{end_div()}}}
{{{begin_div(rightcol40)}}}
[[http://diva.library.cmu.edu/Newell/newell-simon.jpg]]
{{{end_div()}}}
{{{end_div()}}}
#+ATTR_HTML: :class org-center fragment appear
{{{bbold(“推理引擎+领域知识”)}}}
#+BEGIN_NOTES
(Simon & Newell 1957), (Gelenter 1959, Robinson 1965), (Feigenbaum et. al. 1960s), (McCarthy et. al. 1960s, Kowalski 1970s), (Japan, 1980s)
#+END_NOTES
*** “知识工程瓶颈”
#+REVEAL_HTML: <img src="https://www.igcseict.info/theory/7_2/expert/files/stacks_image_5738.png" align=center style="margin:10px auto;"/>
#+ATTR_HTML: :style font-size:3em;position:absolute;right:39.5%;top:21% :class fragment appear :data-fragment-index 1
{{{rawesome(157)}}}
#+ATTR_HTML: :style font-size:2em;position:absolute;right:25%;top:18% :class fragment appear :data-fragment-index 2
{{{rawesome(119)}}}
#+ATTR_REVEAL: :frag (appear appear) :frag_idx (1 2)
- 领域知识成本高;
- 某些知识难以总结、专家不愿分享;

#+ATTR_HTML: :class org-center fragment appear
{{{bbold(解决办法: 通过计算、利用经验来改善系统自身性能)}}}
*** 机器学习
{{{begin_div(flexbox)}}}
{{{begin_div(leftcol40)}}}
[[./figs/dissertation/ml.svg]]
{{{end_div()}}}
{{{begin_div(rightcol60)}}}
\[
\min_f E_{\langle x,y\rangle\sim D}loss(f(x),y)
\]
#+BEGIN_EXPORT html
<div class=flexbox-stretch>
    <img class="fragment appear" data-fragment-index=0 src="https://systweak1.vo.llnwd.net/content/wp/systweakblogsnew/uploads_new/2018/03/hidden-layers-in-network.gif" width=290px height=220px/>
    <img class="fragment appear" data-fragment-index=1 src="https://jeremykun.files.wordpress.com/2017/06/svm_solve_by_hand-e1496076457793.gif" width=280px height=220px/>
    <img class="fragment appear" data-fragment-index=2 src="https://yanndubs.github.io/img/blog/decision-tree-class.gif" width=340px height=220px/>
    <img class="fragment appear" data-fragment-index=3 src="https://cdn-images-1.medium.com/max/1600/1*Nx6IyGfRAV1ly6uDGnVCxQ.gif" width=230px height=220px>
</div>
#+END_EXPORT
{{{end_div}}}
{{{end_div}}}
#+BEGIN_NOTES
机器学习的目标，代表性模型有神经网络、支持向量机、决策树等监督学习方法，以及非监督学习方法
#+END_NOTES
** 机器学习中待解决的问题
{{{begin_div(flexbox)}}}
{{{begin_div(leftcol)}}}
- 可理解性
  - Human Aware AI: [[https://aaai.org/Library/Workshops/ws17-10.php][AAAI'17 WS]], [[http://rakaposhi.eas.asu.edu/haai-aaai/AAAI-Presidential-Address-final.pdf][AAAI'18 Presidential Adress]].
  - Interpretable Machine Learning: [[https://nips.cc/Conferences/2017/Schedule?showEvent=8744][NIPS'17 WS]], [[https://sites.google.com/site/nips2016interpretml/][NIPS'16 WS]].
  - Explainable AI (XAI): [[http://home.earthlink.net/~dwaha/research/meetings/faim18-xai/][IJCAI'18 WS]], [[http://home.earthlink.net/~dwaha/research/meetings/ijcai17-xai/][IJCAI'17 WS]], [[https://www.darpa.mil/program/explainable-artificial-intelligence][DARPA Program]].
  - Human in the Loop Machine Learning: [[https://machlearn.gitlab.io/hitl2017/][ICML'17 WS]].
{{{end_div()}}}
{{{begin_div(rightcol)}}}
{{{img(https://imgs.xkcd.com/comics/machine_learning.png, 100%, auto)}}}
{{{end_div()}}}
{{{end_div()}}}
*** TODO 可理解性的来源
{{{begin_div(flexbox)}}}
{{{begin_div(leftcol)}}}
- 机器学习模型本身的结构
  - 逻辑规则，决策树
  - 图模型
  - 线性模型
  - 核方法
  - 深度神经网络
{{{end_div()}}}
{{{begin_div(rightcol)}}}
- 领域知识的形式
  - 特征
{{{end_div()}}}
{{{end_div()}}}
** 一种可能的解决方案
#+ATTR_HTML: :class org-center
{{{rbold(将机器学习与一阶逻辑表示的知识相结合)}}}

{{{begin_div(flexbox-center,font-size:80%)}}}
#+ATTR_HTML: :class fragment appear :data-fragment-index 1 :style text-align:left
*一阶逻辑* (First-Order Logic, FOL)是一种[[https://zh.wikipedia.org/zh-cn/%25E5%25BD%25A2%25E5%25BC%258F%25E8%25AF%25AD%25E8%25A8%2580][形式语言]].
{{{begin_div(leftcol60)}}}
#+ATTR_REVEAL: :frag (appear appear appear appear appear appear appear) :frag_idx (2 2 2 3 3 4 5)
- 常量: $socrates$
- 变量: $X,Y,\ldots$
- 连词: $\wedge,\vee,\neg,\leftarrow,\ldots$
- {{{rbold(谓词/函数)}}}: $mortal/1$, $man/1$
- {{{rbold(量词)}}}: $\forall$, $\exists$
- 项: $man(socrates)$, $mortal(X)$, $\ldots$
- {{{rbold(公式)}}}: $\forall X(mortal(X)\leftarrow man(X)).$
{{{end_div()}}}
{{{begin_div(rightcol40)}}}
#+ATTR_HTML: :height 400px :class fragment appear :data-fragment-index 1
[[http://www.quickmeme.com/img/a3/a31f18eace4a2930460d85351cb584fc88ff5971bddf657816ce54115d9e7cfd.jpg]]
{{{end_div()}}}
{{{end_div()}}}
#+BEGIN_NOTES
什么是一阶逻辑
#+END_NOTES
*** 引入FOL领域知识的好处
{{{begin_div(flexbox)}}}
{{{begin_div(leftcol60)}}}
{{{begin_div(scalebox90,font-size:0.8em)}}}
{{{bold(目标概念)}}}/{{{rbold(发明谓词)}}}/{{{bbold(领域知识)}}}:
\begin{align}
\mathbf{stair}(P)\leftarrow&\color{#981E32}{\mathbf{s}_1}(P).\\
\mathbf{stair}([X,Y,Z|P])\leftarrow&\color{#981E32}{\mathbf{s}_1}(X,Y,Z)\wedge\mathbf{stair}([Z|P]).\\
\color{#981E32}{\mathbf{s}_1}(X,Y,Z)\leftarrow& \color{#1A53A1}{\mathbf{hori}}(X,Y)\wedge\color{#1A53A1}{\mathbf{vert}}(Y,Z).
\end{align}
{{{end_div()}}}
#+REVEAL_HTML: <img src="./figs/dissertation/stairs.svg" height=300px style="box-shadow:none;position:relative;left:21%"/>
{{{end_div()}}}
{{{begin_div(rightcol40)}}}
#+ATTR_REVEAL: :frag (appear)
1. 良好的{{{rbold(可理解性)}}}
   - 模型形式为 $A\leftarrow B_1\wedge\ldots\wedge B_n$
   - 比统计模型和神经网络更易于人类理解   
2. 强大的{{{rbold(表达能力)}}}
   - 允许递归、谓词发明
   - 强于函数形式表达的优化约束
3. 优秀的{{{rbold(推理能力)}}}
   - 是完备的形式系统
{{{end_div()}}}
{{{end_div()}}}
#+BEGIN_NOTES
完备的，意味着所有真命题都可以通过有限步推理证明；那些不可判定的均为假命题。
#+END_NOTES
*** 引入FOL领域知识的其他好处
{{{begin_div(flexbox)}}}
{{{begin_div(leftcol)}}}
- 可重用性
  - 迁移学习（Transfer Learning）
  - 类比推理（Analogical Reasoning）
  - 学件（Learnware）
  - 通用人工智能（Artificial General Intelligence）
- 训练数据量
  - 强化学习（Reinforcement Learning）
{{{end_div()}}}
{{{begin_div(rightcol)}}}
{{{img(./figs/dissertation/learnware.png, 100%, auto)}}}
{{{end_div()}}}
{{{end_div()}}}
*** TODO FOL知识解决了前面的问题
** 本文工作
{{{begin_div(flexbox-center)}}}
[[./figs/dissertation/chpts.svg]]
{{{end_div()}}}
*** 领域知识辅助机器学习
{{{begin_div(flexbox)}}}
{{{begin_div(leftcol65)}}}
- {{{rbold(任务目标)}}}:
  #+ATTR_REVEAL: :frag (appear appear appear) :frag_idx (1 1 2)
  - FOL领域知识完备{{{rawesome(087)}}}
  - 机器学习模型未训练{{{rawesome(165)}}}
  - 利用领域知识提升机器学习性能
- {{{bbold(以往工作)}}}:
  #+ATTR_REVEAL: :frag (appear appear appear) :frag_idx (4 5 6)
  - 可引入函数表达的领域知识
  - {{{bawesome(188)}}}{{{bbold(难点1)}}}：符号表示数据中采用效率更高的统计学习
  - {{{bawesome(188)}}}{{{bbold(难点2)}}}：非符号表示数据中引入FOL领域知识
- {{{bold(本文工作)}}}:
  #+ATTR_REVEAL: :frag (appear appear) :frag_idx (7 7)
  - 第二章：领域知识增广样本（SUL）
  - 第三章：领域知识辅助约束（LASIN）
{{{end_div()}}}
{{{begin_div(rightcol40)}}}
{{{begin_div(scalebox90,font-size:0.8em)}}}
#+BEGIN_EXPORT html
<img src="./figs/dissertation/lasin/data_1.svg" width=200px height="auto" style="position:fixed;right:20%" class="fragment fade-out" data-fragment-index=3 />
<div class="fragment fade-out" data-fragment-index=3>
<div style="font-size:2em;font-weight:bold;position:fixed;right:38.5%;top:62%;">↓</div>
<div style="position: fixed;top: 71%;right: 5%;">特征学习</div>
</div>
#+END_EXPORT
#+ATTR_REVEAL: :frag fade-in :frag_idx 3
\begin{align}
&\mathbf{char}([S|T])\leftarrow\\
&\hspace{3em}\mathbf{stroke}(S)\wedge\mathbf{strokes}(T).\\
&\mathbf{stroke}([A,B,C|T])\leftarrow\\
&\hspace{3em}\mathbf{ink}(A,B)\wedge\mathbf{ink}(B,C)\\
&\hspace{2em}\wedge\mathbf{angle}(\mathbf{AB},\mathbf{BC})<\alpha\\
&\hspace{2em}\wedge\mathbf{stroke}([B,C|T]).
\end{align}
{{{end_div()}}}
#+BEGIN_EXPORT html
<img class="fragment fade-out" data-fragment-index=3 style="position:relative" width="350" height="auto" src="./figs/dissertation/lasin/origin_dict.svg" />
<img class="fragment fade-in" data-fragment-index=3 style="position:fixed;margin:5px;right:0" width="350" height="auto" src="./figs/dissertation/lasin/stroke_dict.svg" />
#+END_EXPORT
{{{end_div()}}}
{{{end_div()}}}
*** 机器学习驱动领域知识精化
{{{begin_div(flexbox)}}}
{{{begin_div(leftcol65)}}}
- {{{rbold(任务目标)}}}:
  #+ATTR_REVEAL: :frag (appear appear appear) :frag_idx (1 1 2)
  - FOL领域知识不完备{{{rawesome(165)}}}
  - 机器学习模型已训练{{{rawesome(087)}}}
  - 利用机器学习驱动FOL知识精化
- {{{bbold(以往工作)}}}:
  #+ATTR_REVEAL: :frag (appear appear) :frag_idx (4 5)
  - 可在FOL表示的数据中对领域知识进行精化，获得FOL规则集
  - {{{bawesome(188)}}}{{{bbold(难点)}}}：在非符号表示的数据中对FOL领域知识进行精化
- {{{bold(本文工作)}}}:
  #+ATTR_REVEAL: :frag (appear) :frag_idx (6)
  - 第四章：机器学习驱动领域知识精化方法（KRL）
{{{end_div()}}}
{{{begin_div(rightcol40)}}}
{{{begin_div(scalebox90,font-size:0.8em)}}}
#+ATTR_REVEAL: :frag fade-out :frag_idx 3
\begin{align}
&\mathbf{polygon}(p1,[(0,0),(0,1),\ldots]).\\
&\mathbf{polygon}(p2,[(2,4),(0,2),\ldots]).\\
&\ldots\\
&\neg\mathbf{regular}(p1).\\
&\mathbf{regular}(p3).\\
&\ldots
\end{align}
{{{end_div()}}}
#+BEGIN_EXPORT html
<img src="./figs/dissertation/logvis/ex_regular.svg" width=350px height="auto" style="position:fixed;top:25%;right:0%" class="fragment fade-in" data-fragment-index=3 />
<div>
<div style="font-size:1em;font-weight:bold;position:fixed;right:18%;top:47%;">↓</div>
<div style="position:fixed;top:47.5%;right:12%;font-size:0.8em">精化</div>
</div>
#+END_EXPORT
{{{begin_div(scalebox90,font-size:0.8em)}}}
\begin{align}
&\mathbf{regular\_poly_2}(A,G)\leftarrow\\
&\hspace{2em}\mathbf{angles\_list}(A,B)\\
&\hspace{1em}\wedge\mathbf{std\_dev\_bounded}(B,G).\\
&\mathbf{regular\_poly_1}(A,A2)\leftarrow\\
&\hspace{2em}\mathbf{polygon}(A,B)\\
&\hspace{1em}\wedge\mathbf{regular\_poly_2}(B,A2).\\
&\mathbf{regular\_poly}(A)\leftarrow\\
&\hspace{2em}\mathbf{regular\_poly_1}(A,0.02).
\end{align}
{{{end_div()}}}
{{{end_div()}}}
{{{end_div()}}}
*** 领域知识与机器学习互促结合
{{{begin_div(flexbox)}}}
{{{begin_div(leftcol65)}}}
- {{{rbold(任务目标)}}}:
  #+ATTR_REVEAL: :frag (appear appear appear) :frag_idx (1 1 2)
  - FOL领域知识不完备{{{rawesome(165)}}}
  - 机器学习模型未训练{{{rawesome(165)}}}
  - 同时进行FOL知识精化与机器学习
- {{{bawesome(188)}}}{{{bbold(挑战)}}}:
  #+ATTR_REVEAL: :frag (appear appear) :frag_idx (4 4)
  - 训练机器学习模型的标记（label）{{{nl()}}}需要通过逻辑推理获得
  - 逻辑推理使用的基本事实（facts）{{{nl()}}}需要借助机器学习识别
- {{{bold(本文工作)}}}:
  #+ATTR_REVEAL: :frag (appear appear) :frag_idx (5)
  - 第五章：领域知识与机器学习{{{nl()}}}互促结合（反绎学习）
{{{end_div()}}}
{{{begin_div(rightcol40)}}}
{{{begin_div(flexbox-center)}}}
#+BEGIN_EXPORT html
<img src="./figs/dissertation/al/before_learn.svg" class="fragment fade-out" style="box-shadow:none;position:fixed;right:0;" data-fragment-index=3 width=350px height=auto />
<img src="./figs/dissertation/al/after_learn.svg" class="fragment fade-in" style="box-shadow:none;position:fixed;right:0" data-fragment-index=3 width=350px height=auto />
#+END_EXPORT
{{{end_div()}}}
{{{end_div()}}}
{{{end_div()}}}
* 第{{{rbold(2)}}}章{{{nl()}}}{{{nl()}}}一种领域知识增广样本的{{{nl()}}}机器学习方法
** 机器学习中的领域知识
{{{begin_div(flexbox-start)}}}
{{{begin_div(leftcol)}}}
{{{bbold(常见的领域知识：)}}}
#+ATTR_REVEAL: :frag (appear appear) :frag_idx (1 9)
- *类型*:
  #+ATTR_REVEAL: :frag (appear appear appear appear) :frag_idx (2 3 4 8)
  - 模型选择
  - 特征工程  
  - 目标函数
    #+ATTR_REVEAL: :frag (appear appear appear) :frag_idx (5 6 7)
    - 损失函数
    - 正则化项
    - 函数空间约束
  - 先验分布
- *特点*: 一般连续可微
#+ATTR_HTML: :class fragment appear :data-fragment-index 19
{{{bbold(常用于SVM、NN，<br />一般作为凸优化约束)}}}
{{{end_div()}}}
{{{begin_div(rightcol)}}}
{{{rbold(FOL领域知识：)}}}
#+ATTR_REVEAL: :frag (appear appear) :frag_idx (10 18)
- *类型*:
  #+ATTR_REVEAL: :frag (appear appear appear) :frag_idx (11 14 17)
  - 标记间的关系
    #+ATTR_REVEAL: :frag (appear appear appear) :frag_idx (12 13)
    - 层次标记
    - 相关性
  - 特征间的关系
    #+ATTR_REVEAL: :frag (appear appear appear) :frag_idx (15 16)
    - 因果关系
    - 相关性
  - 样本间的关系
- *特点*: 离散，关系型知识
#+ATTR_HTML: :class org-center fragment appear :data-fragment-index 20 :style padding:9px 0 0 0
{{{rbold(常用于统计关系学习，<br />一般作为组合优化约束)}}}
{{{end_div()}}}
{{{end_div()}}}
** 引入FOL领域知识的机器学习方法
{{{begin_div(flexbox-center)}}}
{{{begin_div(leftcol)}}}
#+ATTR_REVEAL: :frag (appear appear) :frag_idx (1 3)
- (概率)归纳逻辑程序((P)ILP){{{bsup([107,34])}}}
  #+ATTR_REVEAL: :frag (appear) :frag_idx (4)
  - 模型为FOL规则
- 统计关系学习(SRL){{{bsup([54])}}}
  #+ATTR_REVEAL: :frag (appear appear) :frag_idx (5 6)
  - 将逻辑关系解释为相关性
  - FOL规则→概率图模型
{{{begin_div(fragment fade-in,,,8)}}}
{{{rbold(难点)}}}
- FOL规则学习和图模型结构学习均为NP难{{{bsup([])}}}
- 只能用于小数据
{{{end_div()}}}
{{{end_div()}}}
{{{begin_div(rightcol fragment fade-in,,,7)}}}
#+ATTR_HTML: :style margin:5px auto;
[[./figs/dissertation/mln.png]]
{{{begin_div(scalebox60 flex-box,flex-direction:column;margin:0;flex-wrap:none auto;width:450px;height:600px)}}}
\begin{align}
\mathbf{friends}(X,Z)\leftrightarrow&\mathbf{friends}(X,Y)\\
&\wedge \mathbf{friends}(Y,Z).\\
\mathbf{cancer}(X)\leftarrow&\mathbf{smokes}(X).\\
\mathbf{smokes}(Y)\leftarrow&\mathbf{smokes}(X)\\
&\wedge\mathbf{friends}(X,Y).\\
\mathbf{smokes}(X)\leftarrow&\neg\mathbf{friends}(X).
\end{align}
{{{begin_div(,width:700px;font-size:1.19em)}}}
#+BEGIN_SRC prolog :exports code
% (P)ILP与SRL使用的关系型数据集
smokes(alan).      smokes(bob).
friends(alan,bob). friends(bob,charlie).
cancer(charlie).
...
?- cancer(jack).
#+END_SRC
{{{end_div()}}}
{{{end_div()}}}
{{{end_div()}}}
{{{end_div()}}}
*** 基于FOL表示的机器学习方法
{{{begin_div(fragment fade-in,,,1)}}}
- 领域知识库 $KB$ 由FOL规则组成:
\[
A\leftarrow B_1\wedge B_2\wedge\ldots\wedge B_n.
\]
{{{end_div()}}}
{{{begin_div(fragment fade-in,,,2)}}}
- 假设模型 $H=(\Gamma,\theta)$ 由（带权）FOL规则组成:
\[
\color{#981E32}{w::}A\leftarrow B_1\wedge B_2\wedge\ldots\wedge B_n.
\]
{{{end_div}}}
#+ATTR_REVEAL: :frag (appear) :frag_idx (3)
- 学习方式:
  1. 更新规则结构 $\Gamma$;
  2. 最大化似然，求解参数 $\theta$;
  3. 若达到终止条件, 输出 $H=(\Gamma,\theta)$; 否则, goto 2.
{{{begin_div(popupbox fragment fade-in,position:absolute;top:40%;left:4%,,4)}}}
*目标函数不可微，无法使用梯度下降等速度较快的{{{nl}}}优化方法求解*
{{{end_div}}}
#+BEGIN_NOTES
由于领域知识和背景知识采用了相同的形式，归纳逻辑程序设计和统计关系学习均直接对这些领域知识进行利用。
#+END_NOTES
*** 常规机器学习方法
{{{begin_div(flexbox-start)}}}
{{{begin_div(leftcol60)}}}
{{{bbold(问题形式化)}}}
{{{begin_div(fragment fade-in,,,0)}}}
- 经验风险最小化
\begin{equation}
\min_{h\in\mathcal{H}}\frac{1}{m}\sum_{i=1}^mL(h(\mathbf{x}_i),y_i),
\end{equation}
{{{end_div()}}}
{{{end_div()}}}
{{{begin_div(rightcol40 fragment fade-in,width:430px,,1)}}}
{{{rbold(符号表示)}}}
{{{begin_div(,font-size:0.8em)}}}
$\mathbf{x}_i\in\mathcal{X}$: 样本与样本空间{{{nl()}}}
$y_i\in\mathcal{Y}$: 标记与标记空间{{{nl()}}}
$h$: 假设模型{{{nl()}}}
$\mathcal{H}$: 假设空间{{{nl()}}}
$L$: 损失函数
{{{end_div()}}}
{{{end_div()}}}
{{{end_div()}}}
#+ATTR_REVEAL: :frag (appear appear appear) :frag_idx (3 4 5)
1. 利用FOL规则初始化神经网络结构{{{bsup([37\,51\,143\,155])}}};
2. 端到端学习，将FOL知识作为序列输入{{{bsup([57\,58\,170])}}};
3. LDA中将FOL知识作为先验分布或后验分布约束，采用SRL类似的方式学习{{{bsup([2\,102])}}}.
{{{begin_div(popupbox-blue fragment fade-in,position:absolute;top:40%;left:8%,,6)}}}
1. *无法进行真正的逻辑推理*;
2. *基于嵌入表示的模型难以泛化至未见数据*;
3. *仅能使用形式十分受限的FOL规则*.
{{{end_div}}}
** 领域知识增广样本
#+ATTR_HTML: :class org-center fragment fade-in :data-fragment-index 1
{{{rbold(将领域知识 $KB$ 增广样本\,<br/>在新的训练数据集中进行学习)}}}
#+ATTR_HTML: :class fragment fade-in :data-fragment-index 2
\begin{equation}
\min_{h\in\color{#981E32}{\mathcal{H}'}}\frac{1}{m}\sum_{i=1}^mL(h(\mathbf{x}_i,\color{#981E32}{\mathbf{u}_i}),y_i),
\end{equation}
#+ATTR_REVEAL: :frag (appear)
- $\color{#981E32}{\mathbf{u}_i}\in\mathcal{F}$: 由$KB\cup\mathcal{x}_i$ 经过逻辑推理而得到的新信息;
- $\color{#981E32}{\mathcal{H}'}$: 增广后的新假设空间.
*** 领域知识增广样本
{{{bbold(SUL学习(Statistical Unfolded Logic Learning))}}}
#+ATTR_REVEAL: :frag (appear)
1. (Unfold): 将 $KB$ 增广为一组新的特征 $F\in\mathcal{F}$，并将原始特征空间增广为 $\mathcal{X}\times\mathcal{F}$;
2. (Learning): 在新空间中进行机器学习；
3. (Folding): 将学得的模型转化为原始问题假设空间中的模型.
*** 领域知识增广样本
{{{begin_div(flexbox-start)}}}
{{{begin_div(leftcol)}}}
{{{bbold(形式化)}}}
{{{begin_div(fragment fade-in,,,1)}}}
\begin{equation}
B\cup KB\vdash F
\end{equation}
{{{end_div()}}}
{{{begin_div(fragment fade-in,font-size:0.8em,,3)}}}
- 通过 $F$ 提取新特征向量 $\mathbf{u}_i=\langle u_{i1},\ldots,u_{ik}\rangle$, 其中:
#+ATTR_HTML: :style margin:20px 0
\begin{equation}
  u_{ij}=\left\{
      \begin{aligned}
        1,\quad&f_j(\mathbf{x}_i)=true,\\
        0,\quad&f_j(\mathbf{x}_i)=false.\\
      \end{aligned}
    \right.
\end{equation}
{{{end_div()}}}
{{{end_div()}}}
{{{begin_div(rightcol fragment fade-in,,,2)}}}
{{{rbold(符号表示)}}}
#+ATTR_HTML: :style font-size:0.8em;line-height:2.3em
$B=\{\mathbf{x}_1,\ldots,\mathbf{x}_m\}$: 样本(背景知识){{{nl()}}}
$KB$: 领域知识{{{nl()}}}
$\vdash$: 逻辑"推出"{{{nl()}}}
$F=\{f_1,\ldots,f_n\}$: 增广特征
{{{end_div()}}}
{{{end_div()}}}
{{{begin_div(remarkbox fragment fade-in,font-size:0.8em,,4)}}}
"$\vdash$"可用前向链推理{{{bsup([71])}}}实现, 它表示根据 $B\cup KB$ 演绎推理得到 $F.$ 因此这些特征 $f_i\in F$ 作为推理的“结论”均至少包含一部分 $KB$ 中的语义.
{{{end_div()}}}
** 实验验证
{{{bbold(任务：统计关系学习)}}}
#+ATTR_REVEAL: :frag (appear) :frag_idx (1)
- *输入:*
{{{begin_div(fragment fade-in,font-size:80%;width:100%,,1)}}}
#+BEGIN_SRC prolog :exports code
% 基本事实（特征）
author(class_1000,author_m_kearns_).    haswordauthor(author_kearns_michael_, word_michael).
author(class_1001,author_m_j_kearns_).  haswordauthor(author_kearns_m_j_, word_j).
haswordtitle(title_an_introduction_to_computational_learning_theory_, word_an).
haswordvenue(venue_proceedings_sigir_, word_sigir).
title(class_187,title_information_prediction_query_by_comittee_).
venue(class_1221,venue_machine_learning_).
% 样例
sameauthor(author_a_blum_,author_avrim_blum_).
samebib(class_693,class_711).
sametitle(title_warmuth_how_to_use_expert_advice_,title_how_to_use_expert_advice_).
samevenue(venue_19th_symposium_theory_computing_,venue_19th_symposium_theory_computing_).
#+END_SRC
{{{end_div()}}}
#+ATTR_REVEAL: :frag (appear) :frag_idx (2)
- *输出:* 带权逻辑规则集 "$w::A\leftarrow B_1\wedge\ldots\wedge B_n$".
*** 领域知识: 关系型路径
#+REVEAL_HTML: <img src="./figs/dissertation/sul_graph.svg" style="box-shadow:none;margin:20px auto;position:fixed;left:5%" width=1024px class="fragment fade-out" data-fragment-index=1 />
#+REVEAL_HTML: <img src="./figs/dissertation/sul_graph1.svg" style="box-shadow:none;margin:20px auto;position:fixed;left:5%" width=1024px class="fragment fade-in" data-fragment-index=1 />
{{{begin_div(,height:310px;width:1024px)}}}
{{{end_div()}}}
{{{begin_div(remarkbox)}}}
*关系型路径假设* {{{bsup([139])}}} 对于所有正样本事实中的逻辑常量，在基本事实构成的超图中总存在长度有限的路径连接它们.
#+ATTR_HTML: :class org-center :style margin:10px auto
\begin{align}
  \forall i,j\in[1..l](\mathbf{t}(X_1,\ldots,X_l)&\leftarrow i\neq j\wedge \mathbf{path}(X_i,X_j)).\\
  \exists \mathbf{R}(\mathbf{path}(X,Y)&\leftarrow \mathbf{R}(X,Z)\wedge \mathbf{path}(Z,Y)).
\end{align}
{{{end_div}}}
*** 实验设置
#+ATTR_REVEAL: :frag (appear)
- {{{bbold(实验数据)}}}
  - *Cora* {{{bsup([134])}}}: 论文引用条目关系, 数据量大、结构较简单;
    - 4个任务: =author=, =bib=, =title=, =venue=
  - *UW-CSW* {{{bsup([36])}}}: 计算机系成员间关系, 数据量较少、结构复杂.
    - 1个任务: =advisedby=
- {{{rbold(对比方法)}}}
  - *RDN-Boost* {{{bsup([116])}}}: Relational Dependency Network
  - *Alchemy* {{{bsup([36])}}}, *ALEPH++-MLN* {{{bsup([66])}}}: 马尔可夫逻辑网
  - *SUL-Path*: 本文方法
    - 使用三种机器学习模型: =J48=, =AdaBoost= 和 =RandomForest=.
*** 实验结果
#+ATTR_HTML: :class org-center :style font-size:0.8em
{{{bbold(AUC值)}}}
#+ATTR_HTML: :style font-size:66%;margin:30px auto
| 算法            | author           | bib              | title            | venue            | advisedby        |
|-----------------+------------------+------------------+------------------+------------------+------------------|
| SUL-Path-J48    | 0.994 ± 0.004   | 0.926 ± 0.025   | 0.921 ± 0.031   | 0.839 ± 0.046   | 0.633 ± 0.071   |
| SUL-Path-Boost  | *0.998 ± 0.002* | *0.998 ± 0.001* | *0.987 ± 0.011* | *0.977 ± 0.011* | 0.975 ± 0.015   |
| SUL-Path-RF     | *0.998 ± 0.002* | 0.989 ± 0.009   | 0.982 ± 0.017   | 0.946 ± 0.023   | *0.992 ± 0.006* |
| RDN-Boost-org   | 0.985 ± 0.014   | 0.916 ± 0.021   | 0.706 ± 0.121   | 0.589 ± 0.040   | N/A              |
| RDN-Boost-total | 0.986 ± 0.012   | 0.949 ± 0.016   | 0.729 ± 0.126   | 0.590 ± 0.038   | 0.983 ± 0.014   |
| Alchemy         | 0.597 ± 0.152   | N/A              | 0.604 ± 0.216   | N/A              | 0.393 ± 0.103   |
| ALEPH++-MLN     | N/A              | N/A              | N/A              | N/A              | 0.127 ± 0.032   |
*** 实验结果
#+ATTR_HTML: :class org-center :style font-size:0.8em
{{{bbold(学习时间(秒))}}}
#+ATTR_HTML: :style font-size:60%;margin:30px auto
| 算法        | author |   bib |  title | venue | advisedby |
|-------------+--------+-------+--------+-------+-----------|
| SUL-J48     |    0.2 |  87.2 |    0.5 |   1.5 |       0.4 |
| SUL-Boost   |    1.1 | 302.6 |    2.2 |   9.5 |       1.1 |
| SUL-RF      |    1.6 | 471.0 |    4.6 |  19.7 |       1.6 |
| RDN-Boost   |   25.6 | 947.8 |  278.6 | 204.1 |      15.6 |
| Alchemy     | 5232.3 |   N/A | 5898.1 |   N/A |    5478.0 |
| ALEPH++-MLN |    N/A |   N/A |    N/A |   N/A |      0.07 |
#+ATTR_HTML: :class org-center :style font-size:0.8em
{{{bbold(SUL增广样本耗时(秒))}}}
#+ATTR_HTML: :style font-size:60%;margin:30px auto
|           |              |     时间开销 |          | 增广样本大小 |
| /         |            < |            > |        < |            > |
|-----------+--------------+--------------+----------+--------------|
| 任务      | 知识转化特征 | 计算特征向量 | 特征维度 |     样本数量 |
|-----------+--------------+--------------+----------+--------------|
| author    |          5.7 |         13.9 |       36 |         2428 |
| bib       |         47.0 |       1475.5 |       64 |       457782 |
| title     |         20.6 |         80.8 |       34 |        16396 |
| venue     |         55.4 |        491.6 |       33 |        73354 |
| advisedby |         11.5 |         22.7 |      452 |        10625 |
*** 实验结果
#+ATTR_HTML: :class org-center :style font-size:0.8em
{{{bbold(领域知识复杂度与学习模型(J48)性能的关系)}}}
{{{begin_div(flexbox)}}}
#+REVEAL_HTML: <img src="./figs/dissertation/sul/title-len.svg" style="box-shadow:none" width=480px />
#+REVEAL_HTML: <img src="./figs/dissertation/sul/uw-len.svg" style="box-shadow:none" width=480px />
{{{end_div()}}}
#+BEGIN_NOTES
增加路径长度确实对提升 SUL-Path 的表现有所帮助.不过一旦长度超过某个阈值，它带来提升就变得十分有限了.领域知识，关系型路径假设：“有限长度路径”的猜想基本是正确的.

由于 SUL-Path-Boost 和 SUL-Path-RF 的结果已经接近最优，我们选择通过观察SUL-Path-J48 的结果来进研究，该结果如图2.2所示。由于领域内的关系过于复杂，advisedby 任务中当最大路径长度超过 4 时，SUL-Path的展开阶段就无法在 1 小时内结束，因此我们没有计算它的结果.
#+END_NOTES
*** 谓词发明
{{{bbold(习得规则)}}}
\begin{align}
  \mathbf{tree_0\_score}(X_1,X_2,W)\leftarrow & \mathbf{ct\_path}_{0,0}(X_1,X_2,N)\\
  &\hspace{1em}\wedge N > 0.0\wedge W=1.0.
\end{align}
{{{rbold(发明谓词)}}}
\begin{align}
  \mathbf{path}_0(X_1,X_2,X_3,X_4,X_5)\leftarrow & \mathbf{author}(X_3,X_1)\wedge \mathbf{title}(X_3,X_4)\\
  &\hspace{1em}\wedge \mathbf{title}(X_5,X_4)\wedge \mathbf{author}(X_5,X_2).\\
  \mathbf{path}_{0,0}(X_1,X_2,X_3,X_4,X_5)\leftarrow & \mathbf{path}_0(X_1,X_2,X_3,X_4,X_5)\\
  &\hspace{1em}\wedge \mathbf{haswordauthor}(X_1,X_6)\\
  &\hspace{1em}\wedge \mathbf{haswordauthor}(X_2,X_6).
\end{align}
{{{begin_div(popupbox fragment fade-in,position:absolute;top:17%;left:7%;font-size:0.8em,,1)}}}
- $path_0$: 作者 $X_1$ 和 $X_2$ 标题一样的论文；
- $path_{0,0}$: $X_1$ 和 $X_2$ 的名字中至少还拥有一个相同的单词.
{{{end_div()}}}
#+BEGIN_NOTES
借助不同新谓词的组合，SUL-Path可以学得拥有较大长度规则，因此能够表示关系较为复杂的目标概念，这在目前其他的SRL和PILP算法中较难做到。
#+END_NOTES
** 小结
- 提出了SUL方法
  - 将FOL领域知识转化为样本
  - 为常规机器学习方法引入FOL知识
  - 与SRL相比效率更高，可实现谓词发明
{{{begin_div(remarkbox fragment fade-in,font-size:60%;padding:0 15px;height:140px;margin:30px auto,,1)}}}
{{{rbold(相关成果已经发表)}}}
- _W.-Z. Dai_ and Z.-H. Zhou. Statistical unfolded logic learning. In: *Proceedings of the 7th Asian Conference on Machine Learning (ACML’15)*, Hong Kong, China, 2015, JMLR: W&CP 45, pp. 349-361. /(CCF-C类, 机器学习领域重要国际会议)/
{{{end_div()}}}
* 第{{{rbold(3)}}}章{{{nl()}}}{{{nl()}}}一种领域知识辅助约束的{{{nl()}}}机器学习方法
** SUL学习的不足
#+ATTR_REVEAL: :frag (appear appear) :frag_idx (1 2)
- 若领域知识不够强，则转化为样本带来的信息量不够;
- SUL学习增广样本被表示为:
  \[
    B\cup KB\color{#981E32}{\vdash}F
  \]
  该过程的"$\vdash$"为基于语构蕴涵(implication)的演绎, 对于某些形式的 $KB$， 该问题{{{red(不可判定)}}}{{{bsup([56])}}}.
  #+ATTR_REVEAL: :frag appear :frag_idx 3
  - 一次性的领域知识增广样本无法保证最优.
#+ATTR_HTML: :class org-center fragment fade-in :data-fragment-index 4
{{{rbold(解决方案：将 $KB$ 作为约束，对学习模型进行迭代优化)}}}
#+BEGIN_NOTES
一旦KB中出现递归或循环规则，便无法通过有限步推出所有满足该式的F. 这也是上一章实验中研究知识复杂度对学习性能影响的原因.
#+END_NOTES
** 领域知识辅助约束
{{{bbold(LASIN (Logical Abduction and Statistical INduction))}}}
\begin{align}
  \min\limits_{h\in\mathcal{H}}\quad&\frac{1}{m}\sum_{i=1}^mL(h(\mathbf{x}_i),y_i).\\
  \text{s.t.}\quad& \text{Con}(KB\cup \mathcal{H}).\nonumber
\end{align}
{{{begin_div(fragment fade-in,,,1)}}}
这里的 $\text{Con}$ 为一致性约束, *"$\models$"* 表示“逻辑满足”:
\[
\not\exists\varphi ((\mathcal{H}\cup KB\models \varphi)\wedge(\mathcal{H}\cup KB\models \neg\varphi))
\]
{{{end_div()}}}
{{{begin_div(fragment fade-in,margin:20px auto,,2)}}}
{{{rbold(难点：)}}}将FOL公式转化为机器学习可利用的约束.
{{{end_div()}}}
#+BEGIN_NOTES
表示学得的假设模型与领域知识不产生矛盾
#+END_NOTES
*** "盲人摸象"
#+REVEAL_HTML: <img src="./figs/dissertation/elephant.jpg" style="margin: 20px auto;" />
#+ATTR_REVEAL: :frag (appear)
1. 通过{{{rbold(反绎逻辑推理)}}}生成一个假设空间;
2. 在假设空间中进行机器学习得到候选模型;
3. 在数据中对候选模型进行检验;
4. 根据检验结果来选择接受、排除或更新自己的假设.
*** 反绎逻辑推理
{{{bbold(三种逻辑推理)}}}:
{{{begin_div(,font-size:0.9em)}}}
\[
B \cup H \models E.
\]
#+ATTR_REVEAL: :frag (appear appear appear) :frag_idx (1 1 2)
1. {{{bold(演绎)}}}: 已知{{{bold(具体)}}}事实 $B$ 与{{{bold(一般)}}}规律 $H$ 推理出{{{bold(具体)}}}结论 $E$;
2. {{{bold(归纳)}}}: 已知{{{bold(具体)}}}事实 $B$ 与{{{bold(具体)}}}样例 $E$ 归纳出{{{bold(一般)}}}规律 $H$;
3. {{{rbold(反绎)}}}: 已知{{{bold(一般)}}}规律 $H$ 与{{{bold(具体)}}}事实 $E$ 推理出{{{bold(具体)}}}原因 $B$.
{{{begin_div(fragment fade-in,,,3)}}}
{{{rbold(演绎与反绎的区别：)}}}
\[
H\equiv B\rightarrow E.
\]
- {{{bold(演绎)}}}是已知前提推出结论;
- {{{rbold(反绎)}}}是根据结论猜测原因.
{{{end_div()}}}
{{{end_div()}}}
*** 反绎逻辑推理
{{{bbold(鞋子湿了)}}}
- 是因为下雨还是草坪喷水器？
{{{begin_div(org-center fragment fade-in,margin:auto,,1)}}}
#+BEGIN_SRC prolog :exports code
% 领域知识 KB
wet_shoes(X) :- wet_grass(X).
wet_grass(X) :- rained_last_night(X).
wet_grass(X) :- sprinkler_was_on(X).

% 完整性约束
rained_last_night(X), sprinkler_was_on(X) ==> fail.

% 观测事实
?- wet_shoes(bob), not(rained_last_night(bob)).

% 反绎推理结果
sprinkler_was_on(bob) ;
false.
#+END_SRC
{{{end_div()}}}
*** 反绎逻辑推理
{{{begin_div(remarkbox)}}}
*定义3.1 (反绎逻辑程序)* 

反绎逻辑程序是一个三元组 $(KB,A,IC)$, 其中 $KB$ 为 *领域知识*, $A$ 为 *可反绎谓词*, $IC$ 为 *完整性约束*. 当给定观测 *事实* $O$ 时, 能够推理出关于 $A$ 的 *具体* 逻辑事实集合 $\Delta$, 使得：

- $\Delta\cup KB\models O$,
- $\Delta\cup KB\models IC$,
- $\text{Con}(KB\cup\Delta)$.
{{{end_div()}}}
{{{begin_div(org-center fragment fade-in,margin:30px auto,,1)}}}
{{{rbold(换言之，$\Delta$ 是基于 $KB$ 对 $O$ 的一种具体“解释”)}}}
{{{end_div}}}
*** 基于反绎推理的假设空间约束
{{{bbold(形式化)}}}
\begin{align}
  &KB\cup\mathcal{H}_{t+1}\models error_t.\\
  \text{s.t.}\quad&\text{Con}(KB\cup\mathcal{H}_{t+1}).\nonumber
\end{align}
{{{begin_div(fragment fade-in,font-size:0.8em;margin:10px auto,,1)}}}
这里的观测事实则为上一轮学得模型 $h_t$ 在训练数据中造成的误差: $error_t=\{\varepsilon_1,\ldots,\varepsilon_m\}$, 其中
\begin{equation}
  \varepsilon_i=\text{difference}(h_{t}(\mathbf{x}_i),y_i),
\end{equation}
{{{end_div()}}}
{{{begin_div(remarkbox fragment fade-in,margin:10px auto,,2)}}}
$\mathcal{H}_{t+1}$ 由反绎推理得出, 因此:
{{{begin_div(,margin:-10px 100px,,2)}}}
1. 它是一组具体逻辑事实构成的集合;
2. 它必定与 $KB$ 一致, 因此满足上面的约束.
{{{end_div()}}}
{{{end_div()}}}
*** LASIN算法
#+ATTR_REVEAL: :frag (appear)
1. 根据 $KB$ 与 $error_{t-1}$ 通过{{{rbold(反绎推理)}}}得到假设空间 $\mathcal{H}_t$;
2. 在 $\mathcal{H}_t$ 中进行机器学习得到候选模型 $h_t$;
3. 在数据中对 $h_t$ 进行检验, 得到误差 $error_t$;
4. 根据检验结果来选择接受、排除或更新 $h_t$.
** 实验验证
{{{bbold(任务：表示学习)}}}{{{bsup([10])}}}
{{{begin_div(fragment fade-in,,,1)}}}
- *输入:* 手写字符图像
#+REVEAL_HTML: <img src="./figs/dissertation/lasin/data_all.svg" width=500px style="margin:20px auto" />
{{{end_div()}}}
#+ATTR_REVEAL: :frag (appear) :frag_idx (2)
- *输出:* 一组特征作为字典，以使得每个原样本在经过该字典编码后所得到的特征向量均为稀疏向量.
#+BEGIN_NOTES
表示学习的假设模型是一组字典，即由具体特征组成的集合
#+END_NOTES
*** 领域知识: 书写原语——“笔划”
{{{begin_div(flexbox)}}}
{{{begin_div(leftcol70)}}}
1. 每个手写字符由多个笔划构成；
2. 每个笔划可看作由一组子笔划（书写原语）构成；
3. 每个子笔划是连续、较光滑的墨迹.
{{{end_div()}}}
{{{begin_div(rightcol30)}}}
#+REVEAL_HTML: <img src="./figs/dissertation/yong.png" style="margin:20px auto" />
{{{end_div()}}}
{{{begin_div(remarkbox fragment fade-in,,,1)}}}
*$KB$*:

\begin{align}
\mathbf{character}(C)\leftarrow & C=\{S_1,S_2,\ldots\}\wedge \mathbf{stroke}(S_1)\wedge\ldots.\\
\mathbf{stroke}(S) \leftarrow & S=\{P_1,P_2,\ldots\}\wedge \mathbf{sub\_strk}(P_1,P_2,P_3)\wedge \mathbf{sub\_strk}(P_2,P_3,P_4)\wedge \cdots.\\
\mathbf{sub\_strk}(A,B,C) \leftarrow &\mathbf{ink}(\mathbf{AB})\wedge\mathbf{ink}(\mathbf{BC})\wedge \mathbf{angle}(\mathbf{AB},\mathbf{BC})<\alpha).
\end{align}
{{{end_div()}}}
*** 学习目标
{{{bbold(基于稀疏编码的特征提取)}}}
\begin{eqnarray}
&&\min_{\mathbf{b},\mathbf{a}}\sum_k{||h^{(k)}-\sum_j{a_j^{(k)}b_j}||_2^2+\beta||a^{(k)}||_1}\label{eq:lasin:si}\\
&\text{s.t. } &||b_j||_2\leq1, \forall j\in 1,\ldots,s.\nonumber\\
&&\forall h^{(k)}((h^{(k)}\in\mathcal{H})\wedge (KB\vDash stroke(h^{(k)})).\nonumber
\end{eqnarray}
{{{rbold(符号表示)}}}
{{{begin_div(,font-size:0.8em)}}}
- $\mathbf{b}$: 待学习字典;
- $\mathbf{a}$: 编码;
- $\mathcal{H}$: 反绎推理得到的假设空间;
- $h^{(k)}$: 假设空间中的具体假设.
{{{end_div()}}}
*** 实验设置
#+ATTR_REVEAL: :frag (appear)
- {{{bbold(实验数据)}}}
  - *MNIST* {{{bsup([95])}}}: 70000张阿拉伯数字图片;
  - *Omniglot* {{{bsup([92])}}}: 1628类字符，每个字符20个样例. 被分为 =OS1= 与 =OS2=;
  - *HPL-Devanagali* {{{bsup([11])}}}: 111种印度语字符，每个字符100个样例.
- {{{rbold(对比方法)}}}
  - *稀疏编码 (SC)* {{{bsup([96])}}}: 未引入任何领域知识.
  - *LASIN-stroke*: 引入关于“笔划”的FOL知识
  - *LASIN-kmeans*: 通过聚类来得到“笔划”
  - *LASIN-spectral*: 通过谱聚类来得到“笔划”
*** 实验结果
#+ATTR_HTML: :class org-center :style font-size:0.8em
{{{bbold(手写字符图片分类精度(%))}}}
#+ATTR_HTML: :style font-size:80%;margin:30px auto
|        |         |         |  $\mid\mathbf{b}\mid = 20$ |          |       |         |  $\mid\mathbf{b}\mid = 50$ |          |
| /      |       < |         |                            |        > |     < |         |                            |        > |
| 数据集 |      SC | stroke  |                     kmeans | spectral |    SC | stroke  |                     kmeans | spectral |
|--------+---------+---------+----------------------------+----------+-------+---------+----------------------------+----------|
| MNIST  |   90.37 | *91.66* |                      90.48 |    90.82 | 93.89 | *94.88* |                      94.47 |    94.79 |
| OS1    | *16.46* | 16.24   |                      15.83 |    15.33 | 21.27 | *21.82* |                      20.92 |    21.09 |
| OS2    |   15.12 | 16.65   |                    *17.37* |    17.43 | 22.61 | *22.90* |                      21.12 |    21.05 |
#+ATTR_HTML: :style font-size:80%;margin:30px auto
|        |         |         | $\mid\mathbf{b}\mid = 100$ |          |       |         | $\mid\mathbf{b}\mid = 200$ |          |
| /      |       < |         |                            |        > |     < |         |                            |        > |
| 数据集 |      SC | stroke  |                     kmeans | spectral |    SC | stroke  |                     kmeans | spectral |
|--------+---------+---------+----------------------------+----------+-------+---------+----------------------------+----------|
| MNIST  |   95.23 | *96.27* |                      96.19 |    96.05 | 95.77 | *97.01* |                      96.91 |    96.97 |
| OS1    |   23.48 | *24.74* |                      23.26 |    23.45 | 25.40 | *26.64* |                      25.94 |    26.37 |
| OS2    |   24.74 | *25.07* |                      24.10 |    23.98 | 25.63 | *26.29* |                      26.00 |    26.21 |
*** 实验结果
#+ATTR_HTML: :class org-center :style font-size:0.8em
{{{bbold(字典迁移分类精度(%))}}}
#+ATTR_HTML: :style font-size:80%;margin:30px auto
|      |       |         | $\mid\mathbf{b}\mid = 20$ |          |       |         | $\mid\mathbf{b}\mid = 50$ |          |
| /    |     < |         |                           |        > |     < |         |                           |        > |
| 任务 |    SC | stroke  |                    kmeans | spectral |    SC | stroke  |                    kmeans | spectral |
|------+-------+---------+---------------------------+----------+-------+---------+---------------------------+----------|
| M2O  | 19.36 | *20.69* |                     19.09 |    18.60 | 20.39 | *23.38* |                     20.86 |    21.09 |
| O2M  | 87.72 | *88.14* |                     87.98 |    88.06 | 93.08 | *93.69* |                     93.44 |    93.67 |
#+ATTR_HTML: :style font-size:80%;margin:30px auto
|        |       |         | $\mid\mathbf{b}\mid = 100$ |          |       |         | $\mid\mathbf{b}\mid = 200$ |          |
| /      |     < |         |                            |        > |     < |         |                            |        > |
| 数据集 |    SC | stroke  | kmeans                     | spectral |    SC | stroke  |                     kmeans | spectral |
|--------+-------+---------+----------------------------+----------+-------+---------+----------------------------+----------|
| M2O    | 20.43 | *23.19* | 20.93                      |    21.48 | 18.62 | *24.63* |                      22.57 |    22.82 |
| O2M    | 96.03 | 96.00   | *96.11*                    |    95.81 | 96.11 | *96.39* |                      96.19 |    96.22 |
*** 实验结果
#+ATTR_HTML: :class org-center :style font-size:0.8em
{{{bbold(印度语手写字符原语学习结果)}}}
#+REVEAL_HTML: <img src="./figs/dissertation/lasin/dev_all.svg" style="margin:-22px auto;box-shadow:none"/>
** 小结
- 提出了LASIN方法
  - 将FOL领域知识嵌入机器学习优化过程;
  - 将FOL领域知识转化转化为对假设空间的约束.
{{{begin_div(remarkbox fragment fade-in,font-size:60%;padding:0 15px;height:180px;margin:30px auto,,1)}}}
{{{rbold(相关成果已经发表)}}}
- _W.-Z. Dai_ and Z.-H. Zhou. Combining logic abduction and statistical induction: Discovering written primitives with human knowledge. In: *Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI’17)*, San Francisco, CA, 2017, pp.4392-4398. /(CCF-A类会议)/
{{{end_div()}}}
* 第{{{rbold(4)}}}章{{{nl()}}}{{{nl()}}}一种机器学习驱动的{{{nl()}}}领域知识精化方法
** 领域知识精化
- {{{bbold(目标)}}}: 从数据中学习一组描述目标概念的逻辑规则.
{{{begin_div()}}}
\begin{equation}
B\cup H\models E.
\end{equation}
{{{end_div()}}}
- {{{rbold(常见方法:)}}}
  - 归纳逻辑程序设计(ILP) {{{bsup([107])}}};
    - 概率归纳逻辑程序设计(PILP) {{{bsup([34])}}};
  - 统计关系学习(SRL) {{{bsup([54])}}}
{{{begin_div(popupbox-blue fragment fade-in,width:76%;font-weight:bold,,1)}}}
{{{p(0)}}}实际任务中大量存在非逻辑符号表示的数据,{{{nl}}} 以上方法均难以适用.{{{wsup([145\,16])}}}{{{ep}}}
{{{end_div()}}}
{{{begin_div(remarkbox,position:absolute;width:300px;top:48%;right:0)}}}
{{{p(0)}}}此类方法中的 $B$, $H$ 和 $E$ 均为FOL公式与事实.{{{ep}}}
{{{end_div()}}}
** 机器学习驱动知识精化
{{{bbold(形式化)}}}
\[
KB\cup \color{#981E32}{D}\cup\{\color{#981E32}{f}\}\cup H \models \color{#981E32}{E}.
\]
{{{begin_div(fragment fade-in,,,1)}}}
{{{rbold(符号表示)}}}
{{{begin_div(,font-size:0.8em)}}}
#+ATTR_REVEAL: :frag (appear appear appear appear appear) :frag_idx (2 2 3 4 5)
- $KB$: 领域知识;
- $H$: 假设模型;
- {{{rbold($D$)}}}: $\{\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_m\}$ 为训练样本, 其中 $\mathbf{x}_i\in\mathcal{X}$, 而 $\mathcal{X}$ 为 *非符号表示* 的输入空间, 如图像、波形、向量等等.
- {{{rbold($E$)}}}: $\{y_1,y_2,\ldots,y_m\}$ 为每个样例的标记;
- {{{rbold($f:\mathcal{X}\mapsto \Gamma$)}}}: 可从 $D$ 中提取基本事实的 *机器学习模型*, 其中 $\Gamma$ 表示 $KB$ 的Herbrand域{{{bsup([43])}}}.
{{{end_div}}}
{{{end_div}}}
*** KRL方法
{{{bbold(Knowledge Refinement by Learning)}}}
{{{begin_div(fragment fade-in,,,1)}}}
- 使用机器学习模型从样本中提取必要的逻辑事实 $F$，以对 $KB$ 进行增广.
{{{begin_div()}}}
\[
KB\cup \color{#981E32}{F}\models D
\]
{{{end_div()}}}
{{{end_div()}}}
{{{begin_div(fragment fade-in,,,3)}}}
{{{rbold($F$ 为 $D$ 中采样得到的具体事实，可通过反绎推理获取.)}}}
{{{end_div()}}}
{{{begin_div(fragment fade-in,,,2)}}}
- 对增广后的 $KB\cup F$ 使用基于FOL表示的机器学习方法进行知识精化.
{{{begin_div()}}}
\[
KB\cup F\cup\color{#981E32}{H}\models E
\]
{{{end_div()}}}
{{{end_div()}}}
{{{begin_div(fragment fade-in,,,4)}}}
{{{rbold($H$ 为FOL规则，可通过归纳推理学得.)}}}
{{{end_div()}}}
*** 基于机器学习的领域知识增广
- {{{rbold(目标)}}}：使用机器学习模型从样本中提取必要的逻辑事实，以对 $KB$ 进行增广.
{{{begin_div()}}}
\[
KB\cup \color{#981E32}{F}\models D
\]
{{{end_div()}}}
{{{begin_div(fragment fade-in,,,1)}}}
- {{{bbold(难点)}}}: $D$ 为 *非符号表示* 的样本, 无法直接用于反绎推理.
{{{begin_div(remarkbox fragment fade-in,,,2)}}}
对每个样本 $\mathbf{x}_i\in D$ 利用机器学习模型 $f$ 对 $D$ 进行采样，以获得具体逻辑事实 $F$.
{{{begin_div(,width:100%)}}}
\begin{equation}
KB\cup F\models f(\mathbf{x}_i)
\end{equation}
{{{end_div()}}}
{{{begin_div(fragment fade-in,width:100%,,3)}}}
可根据反绎推理（主动假设检验）的特点, 实现按需采样.
{{{end_div()}}}
{{{end_div()}}}
{{{end_div()}}}
*** 基于机器学习的领域知识精化
- {{{rbold(目标)}}}：对增广后的 $KB\cup F$ 使用基于FOL表示的机器学习方法进行知识精化.
{{{begin_div()}}}
\[
KB\cup F\cup\color{#981E32}{H}\models E
\]
{{{end_div()}}}
{{{begin_div(remarkbox fragment fade-in,margin:20px auto,,2)}}}
*元解释学习(Meta-Interpretive Learning)* {{{bsup([114])}}}
{{{begin_div(,width:100%)}}}
 \begin{align*}
          &prove([], Prog, Prog)\leftarrow.\\
          &prove([Atom|As], Prog1, Prog2)\leftarrow\\
          &\quad metarule(Name,MetaSub, (Atom\leftarrow Body), Order)\\
          &\quad\wedge Order\\
          &\quad\wedge save\_subst(metasub(Name,MetaSub), Prog1, Prog3)\\
          &\quad\wedge prove(Body, Prog3, Prog4)\\
          &\quad\wedge prove(As, Prog4, Prog2).
\end{align*}
{{{end_div()}}}
{{{end_div()}}}
** 实验验证
{{{bbold(任务: 视觉概念精化)}}}
{{{begin_div(fragment fade-in,,,1)}}}
- *输入:*
#+REVEAL_HTML: <img src="./figs/dissertation/logvis/ex_regular.svg" width=500px style="margin:20px auto" />
{{{end_div()}}}
#+ATTR_REVEAL: :frag (appear appear) :frag_idx (2 3)
- *输出:* 一组逻辑规则, 形式化描述图片中的目标概念
  - 如“正多边形是内角全部相等的多边形”.
*** 机器学习模型与领域知识
- *机器学习模型*: 采样图中的 "点".
  - 无噪声: Sobel算子
  - 有噪声: 前/背景分割模型{{{bsup([176])}}}
{{{begin_div(fragment fade-in,,,1)}}}
- *领域知识*: 基本几何概念（点、线、面、圆、角度、亮度等等）, 实现按需采样的反绎逻辑程序.
{{{begin_div(remarkbox)}}}
\begin{align}
\mathbf{polygon}(Pol_i,[Edge_1,\ldots,Edge_n])&\leftarrow\mathbf{edge}(Edge_1)\wedge\ldots\wedge \mathbf{edge}(Edge_n)\nonumber\\
  &\hspace{-3em}\wedge \mathbf{connected}(Edge_1,Edge_2)\wedge\ldots\wedge \mathbf{connected}(Edge_n,Edge_1).
\end{align}
{{{end_div()}}}
{{{end_div()}}}
#+REVEAL_HTML: <img src="./figs/dissertation/logvis/fit-tri.svg" style="height: 200px;margin:10px auto;box-shadow:none;" class="fragment fade-in" data-fragment-index=2 />
*** 实验设置
- {{{bbold(实验数据)}}}
#+REVEAL_HTML: <img src="./figs/dissertation/logvis/data-all.svg" style="height:340px;margin:10px auto;box-shadow:none" />
{{{begin_div(fragment fade-in,,,1)}}}
- {{{rbold(目标概念)}}}
  - 多边形定义: =triangle=, =quadrilateral=, =pentagon=, =hexagon=, =regular=, =right_triangle=
  - 光源与位置: =light_source=, =light_source_angle=
{{{end_div()}}}
*** 实验结果
{{{bbold(三角形概念)}}}
{{{begin_div(,font-size:0.9em)}}}
\begin{align}
  \mathbf{triangle_2}(A,C,H)&\leftarrow \mathbf{rmv\_rdndnt}(A,B,C)\wedge \mathbf{\color{#981E32}{list\_length}}(B,H).\\
  \mathbf{triangle_1}(A,A2,B2)&\leftarrow \mathbf{polygon}(A,B)\wedge \mathbf{triangle\_2}(B,A2,B2).\\
  \mathbf{triangle}(A)&\leftarrow \mathbf{triangle_0}(A,0.04,\mathbf{\color{#981E32}{3}}).\\
  \mathbf{triangle_1}(A,G)&\leftarrow \mathbf{polygon}(A,B)\wedge \mathbf{list\_length}(B,G).\\
  \mathbf{triangle}(A)&\leftarrow \mathbf{triangle}_1(A,\mathbf{\color{#981E32}{3}}).
\end{align}
{{{begin_div()}}}
{{{begin_div(fragment fade-in,,,1)}}}
{{{bbold(正多边形概念)}}}
{{{begin_div(,font-size:0.8em)}}}
\begin{align}
  \mathbf{regular\_poly_2}(A,G)&\leftarrow \mathbf{angles\_list}(A,B)\wedge \mathbf{\color{#981E32}{std\_dev\_bounded}}(B,G).\\
  \mathbf{regular\_poly_1}(A,A2)&\leftarrow \mathbf{polygon}(A,B)\wedge \mathbf{regular\_poly_2}(B,A2).\\
  \mathbf{regular\_poly}(A)&\leftarrow \mathbf{regular\_poly_1}(A,\mathbf{\color{#981E32}{0.02}})
\end{align}
{{{end_div()}}}
{{{end_div()}}}
{{{begin_div(popupbox fragment fade-in,width:800px;;padding:30px;position:absolute;top:40%;left:7%,,2)}}}
与其他图像识别方法相比，习得模型不仅更具可理解性，在图片分类精度上也更高.
{{{end_div()}}}
#+BEGIN_NOTES
的确学会了多边形概念的形式化定义.
#+END_NOTES
*** 实验结果
{{{bbold(天文与显微镜图片中的光照角度)}}}

\begin{align}
\mathbf{clock\_angle}(A,B,C)&\leftarrow \mathbf{clock\_angle_1}(A,B,D)\\
&\wedge\mathbf{\color{#981E32}{light\_source\_angle}}(A,D,C).\\
\mathbf{clock\_angle_1}(A,B,C)&\leftarrow \mathbf{highlight}(A,B)\wedge \mathbf{clock\_angle_2}(A)\\
&\wedge\mathbf{clock\_angle_3}(C).\\
\mathbf{\color{#981E32}{clock\_angle_2}}(Obj).&\\
\mathbf{\color{#981E32}{clock\_angle_3}}(Light).&
\end{align}

{{{begin_div(popupbox fragment fade-in,width:560px;font-size:0.8em;padding: 10px 30px;,,1)}}}
*谓词发明*:
- $\mathbf{clock\_angle_2}$: 反光面凹凸性;
- $\mathbf{clock\_angle_3}$: 光源名称.
{{{end_div}}}
*** 模型重用
{{{bbold(直角三角形概念(重用三角形概念))}}}

{{{begin_div(,font-size:0.8em)}}}
\begin{align}
  \mathbf{right\_tri_3}(A,G,H)&\leftarrow \mathbf{angles\_list}(A,B)\wedge \mathbf{has\_angle}(B,G,H).\\
  \mathbf{right\_tri_2}(A,A2,B2)&\leftarrow \mathbf{polygon}(A,B)\wedge \mathbf{right\_tri_3}(B,A2,B2).\\
  \mathbf{right\_tri_1}(A,A2,B2)&\leftarrow \mathbf{right\_tri_2}(A,A2,B2)\wedge \mathbf{\color{#981E32}{triangle}}(A).\\
  \mathbf{right\_tri}(A)&\leftarrow \mathbf{right\_tri_1}(A,0.5,0.015).
\end{align}
{{{end_div()}}}
{{{begin_div(fragment fade-in,,,1)}}}
{{{bbold(环形山光照歧义(重用光照角度概念))}}}
#+REVEAL_HTML: <img src="./figs/dissertation/logvis/ambi.svg" style="margin:-15px auto;box-shadow:none;" height=300px/>
{{{end_div()}}}
#+BEGIN_NOTES
得益于FOL的表达能力，基于逻辑表示学习的模型能够将学得的模型可以作为领域知识被直接重用于其他任务。

一个有趣的例子便是将光照角度的概念重用在类型完全不同的图像里。
#+END_NOTES
** 小结
- 提出了KRL方法
  - 利用机器学习模型对FOL领域知识进行增广;
  - 需求数据量少，学得模型能够直接重用.
{{{begin_div(remarkbox fragment fade-in,font-size:60%;padding:0 15px;height:360px;margin:30px auto,,1)}}}
{{{rbold(相关成果已经发表)}}}
- _W.-Z. Dai_, S. H. Muggleton, J. Wen, A. Tamaddoni-Nezhad and Z.-H. Zhou. Logical vision: One-Shot Meta-Interpretive Learning on Real Images. In: *Proceedings of the 27th International Conference on Inductive Logic Programming (ILP’17)*, Orleans, France, 2017, pp.46-62. /(CCF-C类, 归纳逻辑程序设计领域旗舰会议)/
- _W.-Z. Dai_, S. H. Muggleton, and Z.-H. Zhou. Logical vision: Meta-interpretive learning for simple geometrical concepts. In: *Proceedings of the 25th International Conference on Inductive Logic Programming (ILP’15)*. Kyoto, Japan, 2015, pp.1-16. /(CCF-C类, 归纳逻辑程序设计领域旗舰会议)/
- S. H. Muggleton, _W.-Z. Dai_, C. Sammut, A. Tamaddoni-Nezhad, J. Wen, and Z.-H. Zhou. Meta-interpretive learning from noisy images. *Machine Learning*. 107(7): 749-766, 2018. (机器学习领域重要国际期刊)
{{{end_div()}}}
* 第{{{rbold(5)}}}章{{{nl()}}}{{{nl()}}}一种领域知识与机器学习{{{nl()}}}互促结合框架
** 领域知识与机器学习互促结合
** Neural Logical Machine
** 实验验证
** 小结
* 总结
